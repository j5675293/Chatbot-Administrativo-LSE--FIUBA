# Docker Compose - Chatbot Administrativo LSE-FIUBA
# Autor: Juan Ruiz Otondo - CEIA FIUBA
#
# Uso:
#   docker-compose up -d          # Iniciar todos los servicios
#   docker-compose up api          # Solo API
#   docker-compose logs -f api     # Ver logs
#   docker-compose down            # Detener todo

version: "3.8"

services:
  # ── Ollama LLM Server ──────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: chatbot-lse-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ── Descargar modelo LLM ───────────────────────────────────
  ollama-pull:
    image: ollama/ollama:latest
    container_name: chatbot-lse-ollama-pull
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["ollama", "pull", "llama3"]
    environment:
      - OLLAMA_HOST=http://ollama:11434
    restart: "no"

  # ── API FastAPI ────────────────────────────────────────────
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chatbot-lse-api
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env:ro
    environment:
      - PYTHONPATH=/app
      - OLLAMA_BASE_URL=http://ollama:11434
      - LLM_BACKEND=ollama
      - LLM_MODEL=llama3
    depends_on:
      ollama:
        condition: service_healthy
    command: ["python", "run_api.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ── Streamlit UI ───────────────────────────────────────────
  ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chatbot-lse-ui
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env:ro
    environment:
      - PYTHONPATH=/app
      - API_URL=http://api:8000
    depends_on:
      api:
        condition: service_healthy
    command: >
      streamlit run src/ui/app.py
      --server.port=8501
      --server.address=0.0.0.0
      --server.headless=true
    restart: unless-stopped

  # ── Pipeline (ejecución única) ─────────────────────────────
  pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chatbot-lse-pipeline
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env:ro
    environment:
      - PYTHONPATH=/app
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    command: ["python", "run_pipeline.py"]
    profiles:
      - pipeline

volumes:
  ollama_data:
    driver: local
