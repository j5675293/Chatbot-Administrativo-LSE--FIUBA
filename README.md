# ü§ñ ASESOR ADMINISTRATIVO INTELIGENTE MEDIANTE PROCESAMEINTO DE LENGUAJE NATURAL - POSGRADO LSE-FIUBA

<img width="1875" height="866" alt="IA -LSE" src="https://github.com/user-attachments/assets/bf393332-4cd2-4bae-b878-e256184c3493" />

**Trabajo Final** de la Carrera de Especializaci√≥n en Inteligencia Artificial
Laboratorio de Sistemas Embebidos (LSE) - Facultad de Ingenier√≠a - Universidad de Buenos Aires

**Autor:** Juan Ruiz Otondo - a1702

---

## Descripci√≥n

Agente administrativo inteligente (chatbot) basado en Procesamiento de Lenguaje Natural (PLN) para la unidad de Posgrado del Laboratorio de Sistemas Embebidos (LSE) de FIUBA. El sistema responde consultas de la comunidad universitaria sobre reglamentos, carreras de especializaci√≥n (CEIA, CESE, CEIoT), maestr√≠as (MIA, MIAE, MIoT, MCB), procesos administrativos y preguntas frecuentes.
El sistema implementa una arquitectura en 5 capas que combina t√©cnicas avanzadas de RAG (Retrieval-Augmented Generation) con GraphRAG y mecanismos anti-alucinaci√≥n de triple verificaci√≥n para garantizar respuestas precisas y verificables.

### Caracter√≠sticas principales

- **RAG Vectorial (FAISS):** Retrieval-Augmented Generation con base de datos vectorial para b√∫squeda sem√°ntica
- **GraphRAG (NetworkX):** Grafo de conocimiento con entidades acad√©micas y sus relaciones
- **Sistema H√≠brido:** Combinaci√≥n inteligente de ambos sistemas con routing basado en tipo de consulta
- **HyDE (Hypothetical Document Embeddings):** Genera documentos hipot√©ticos para mejorar el retrieval
- **Query Expansion:** Expansi√≥n autom√°tica de consultas con sin√≥nimos del dominio y LLM
- **Anti-alucinaci√≥n multi-capa:** Verificaci√≥n de fidelidad, abstenci√≥n, cross-referencing
- **Memoria conversacional:** Ventana deslizante con resumen progresivo y contextualizaci√≥n de queries
- **Feedback Human-in-the-Loop:** Sistema de valoraci√≥n y mejora continua basada en usuarios
- **M√©tricas RAGAS:** Evaluaci√≥n con faithfulness, answer relevance, context precision y recall
- **Citaciones autom√°ticas:** Trazabilidad completa con fuentes y secciones
- **Pipeline automatizado:** Procesamiento incremental de nuevos documentos
- **Evaluaci√≥n comparativa:** Framework de testing RAG vs GraphRAG vs Hybrid
- **Analytics Dashboard:** Visualizaci√≥n de m√©tricas y feedback del sistema
- **Docker Compose:** Despliegue completo con un solo comando

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Interfaz Streamlit (+ Analytics Dashboard)             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    API FastAPI + Feedback                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              ‚îÇ   Answer     ‚îÇ                                   ‚îÇ
‚îÇ   Hybrid     ‚îÇ Synthesizer  ‚îÇ   Anti-Hallucination Engine       ‚îÇ
‚îÇ  Retriever   ‚îÇ  + Citation  ‚îÇ   (Faithfulness + Abstention +    ‚îÇ
‚îÇ              ‚îÇ   Manager    ‚îÇ    Cross-Reference)               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      ‚îÇ              ‚îÇ                                           ‚îÇ
‚îÇ  RAG ‚îÇ   GraphRAG   ‚îÇ   Query Enhancement                      ‚îÇ
‚îÇ FAISS‚îÇ  NetworkX    ‚îÇ   (HyDE + Query Expansion)               ‚îÇ
‚îÇ      ‚îÇ              ‚îÇ                                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    Conversation Memory          LLM Provider                    ‚îÇ
‚îÇ  (Window + Summary)          (Ollama / OpenAI)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              Data Pipeline                                       ‚îÇ
‚îÇ  PDF Extraction ‚Üí Cleaning ‚Üí Chunking ‚Üí Metadata                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          Evaluation (RAGAS + Benchmark + Feedback)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Flujo de Procesamiento de Consulta

```mermaid
graph TD
    A[üë§ Usuario ingresa consulta] --> B[üñ•Ô∏è Streamlit UI + Analytics]
    B -->|HTTP POST| C[‚ö° FastAPI]
    
    C --> D1[üìù Conversation Memory]
    C --> D2[üîç Query Enhancement]
    
    D1 -->|Contexto hist√≥rico| E[Reformulaci√≥n contextual]
    D2 -->|HyDE + Expansions| E
    
    E --> F{üîÄ Hybrid Retriever}
    
    F -->|Paralelo| G1[üîç RAG/FAISSQuery + HyDE + Expansions]
    F -->|Paralelo| G2[üï∏Ô∏è GraphRAG/NetworkXEntities + Relations]
    
    G1 --> H[üìä Reciprocal Rank FusionPesos Adaptativos]
    G2 --> H
    
    H --> I[‚úçÔ∏è Answer Synthesizer+ Citation Manager]
    
    I --> J[üõ°Ô∏è Anti-Hallucination Engine]
    
    J --> K1[‚úÖ Faithfulness CheckNLI Score ‚â• 0.75]
    J --> K2[üîó Cross-Reference0 conflicts]
    J --> K3[üìä AbstentionConfidence ‚â• 0.65]
    
    K1 --> L{Pass All 3?}
    K2 --> L
    K3 --> L
    
    L -->|S√≠| M[‚úÖ Respuesta Aprobada]
    L -->|No| N[üö´ Abstenci√≥n Honesta]
    
    M --> O[üñ•Ô∏è Streamlit Renderiza]
    N --> O
    
    O --> P[üëç Feedback del Usuario]
    P -->|Rating ‚â§ 2| Q[üìà Failure Analysis‚Üí Test Set]
    P -->|Rating ‚â• 4| R[‚úÖ Success Tracking]
    
    style F fill:#FFF3E0
    style G1 fill:#E3F2FD
    style G2 fill:#E8F5E9
    style J fill:#FFEBEE
    style L fill:#FFF9C4
    style M fill:#C8E6C9
    style N fill:#FFCDD2
```


## Estructura del proyecto

```
chatbot-lse-posgrados/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Configuraci√≥n centralizada (Pydantic)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ processed/               # Chunks procesados (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ indexes/                 # √çndice FAISS
‚îÇ   ‚îú‚îÄ‚îÄ graphs/                  # Grafo de conocimiento (GraphML + Pickle)
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/              # Reportes + Feedback
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_pipeline/           # Pipeline de procesamiento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_extractor.py     # Extracci√≥n dual (PyMuPDF + pdfplumber)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_cleaner.py      # Normalizaci√≥n en espa√±ol
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker.py           # Chunking multi-estrategia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_extractor.py# Extracci√≥n de metadata acad√©mica
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline_orchestrator.py # Orquestador con detecci√≥n de cambios
‚îÇ   ‚îú‚îÄ‚îÄ rag/                     # RAG Vectorial
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py        # Sentence-Transformers multilingual
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py      # FAISS IndexFlatIP + MMR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py         # Retriever con cross-encoder reranking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_chain.py         # Cadena RAG completa
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hyde.py              # HyDE - Hypothetical Document Embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ query_expansion.py   # Expansi√≥n de queries con LLM y sin√≥nimos
‚îÇ   ‚îú‚îÄ‚îÄ graph_rag/               # GraphRAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entity_extractor.py  # Extracci√≥n de 10 tipos de entidades
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ relationship_mapper.py # 11 tipos de relaciones acad√©micas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph_builder.py     # Constructor de grafo NetworkX
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ graph_retriever.py   # Retrieval basado en grafo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ community_detector.py# Detecci√≥n de comunidades (Louvain)
‚îÇ   ‚îú‚îÄ‚îÄ hybrid/                  # Sistema h√≠brido
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_retriever.py  # Combinaci√≥n RAG + GraphRAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anti_hallucination.py# Motor anti-alucinaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ citation_manager.py  # Gesti√≥n de citaciones
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ answer_synthesizer.py# S√≠ntesis de respuesta final
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conversation_memory.py # Memoria conversacional
‚îÇ   ‚îú‚îÄ‚îÄ llm/                     # Proveedores LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_provider.py      # Abstracci√≥n Ollama/OpenAI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py           # Templates en espa√±ol
‚îÇ   ‚îú‚îÄ‚îÄ api/                     # API REST
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py              # App FastAPI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py           # Modelos Pydantic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py      # Inyecci√≥n de dependencias
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ chat.py          # /chat, /chat/compare, /feedback
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ health.py        # /health y /stats
‚îÇ   ‚îú‚îÄ‚îÄ ui/                      # Interfaz
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py               # Aplicaci√≥n Streamlit principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ analytics.py     # Dashboard de analytics
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/              # Evaluaci√≥n
‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py         # Evaluador comparativo + RAGAS
‚îÇ       ‚îú‚îÄ‚îÄ test_sets.py         # Conjunto de preguntas con ground truth
‚îÇ       ‚îú‚îÄ‚îÄ ragas_metrics.py     # M√©tricas RAGAS
‚îÇ       ‚îî‚îÄ‚îÄ feedback.py          # Sistema de feedback
‚îú‚îÄ‚îÄ tests/                       # Tests unitarios e integraci√≥n
‚îú‚îÄ‚îÄ Dockerfile                   # Imagen Docker
‚îú‚îÄ‚îÄ docker-compose.yml           # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ run_pipeline.py              # Ejecutar pipeline de datos
‚îú‚îÄ‚îÄ run_api.py                   # Lanzar API
‚îú‚îÄ‚îÄ run_app.py                   # Lanzar interfaz Streamlit
‚îú‚îÄ‚îÄ run_evaluation.py            # Ejecutar evaluaci√≥n comparativa
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias
‚îú‚îÄ‚îÄ pytest.ini                   # Configuraci√≥n de tests
‚îî‚îÄ‚îÄ .env.example                 # Variables de entorno template
```

## üîÑ Flujo de Procesamiento de Consulta

```mermaid
graph TD
    A[üë§ Usuario ingresa consulta] --> B[üñ•Ô∏è Streamlit UI]
    B -->|HTTP POST| C[‚ö° FastAPI /chat]
    C --> D{üîÄ Hybrid Retriever}
    
    D -->|Paralelo| E[üîç RAG/FAISSB√∫squeda Vectorial]
    D -->|Paralelo| F[üï∏Ô∏è GraphRAG/NetworkXB√∫squeda en Grafo]
    
    E --> G[üìä Fusi√≥n RRF]
    F --> G
    
    G --> H[‚úçÔ∏è Answer Synthesizer]
    H --> I[üìö Citation Manager]
    I --> J[üõ°Ô∏è Anti-Hallucination Engine]
    
    J -->|Faithfulness| K{‚úÖ Score ‚â• 0.6?}
    K -->|S√≠| L[üì§ Respuesta con citas]
    K -->|No| M[üö´ Abstenci√≥n honesta]
    
    L --> N[üñ•Ô∏è Streamlit renderiza]
    M --> N
    N --> O[üë§ Usuario recibe respuesta]
    
    style D fill:#FFF3E0
    style E fill:#E3F2FD
    style F fill:#E8F5E9
    style J fill:#FFEBEE
    style K fill:#FFF9C4
```

## üõ†Ô∏è Stack Tecnol√≥gico

### **Backend & Core**
| Componente | Tecnolog√≠a | Prop√≥sito |
|------------|------------|-----------|
| Framework API | `FastAPI` + `Uvicorn` | Servicios REST as√≠ncronos |
| Validaci√≥n | `Pydantic` | Schemas y configuraci√≥n |
| Embeddings | `Sentence-Transformers` | Vectorizaci√≥n sem√°ntica multiling√ºe |
| Vector Search | `FAISS` | B√∫squeda de similitud ultra-r√°pida |
| Graph Analysis | `NetworkX` | An√°lisis de grafo de conocimiento |
| Community Detection | `Louvain` | Clustering tem√°tico |
| Re-ranking | `Cross-Encoder` | Refinamiento de resultados |
| NLI | `DeBERTa-v3` | Verificaci√≥n de fidelidad |
| PDF Processing | `PyMuPDF` + `pdfplumber` | Extracci√≥n dual de PDFs |
| OCR | `Tesseract` | Documentos escaneados |

### **LLM Providers**
| Modo | Proveedor | Modelos |
|------|-----------|---------|
| Local | `Ollama` | Llama 3.1 (70B), Mistral 7B |
| Cloud | `OpenAI` | GPT-4, GPT-4 Turbo |

### **Frontend**
| Componente | Tecnolog√≠a |
|------------|------------|
| UI Framework | `Streamlit` |
| HTTP Client | `requests` |

### **Testing & Quality**
| Componente | Tecnolog√≠a |
|------------|------------|
| Testing | `pytest` |
| Coverage | `pytest-cov` |
| Type Checking | `mypy` |

## üìä Mapeo Arquitectura ‚Üí C√≥digo

| Capa Arquitect√≥nica | Directorio/M√≥dulo | Archivos Principales |
|---------------------|-------------------|----------------------|
| **Capa 1**: Interfaz | `src/ui/` | `app.py`, `run_app.py` |
| **Capa 2**: API | `src/api/` | `main.py`, `schemas.py`, `routes/*`, `run_api.py` |
| **Capa 3**: Core | `src/rag/`<br/>`src/graph_rag/`<br/>`src/hybrid/` | `hybrid_retriever.py`<br/>`anti_hallucination.py`<br/>`answer_synthesizer.py` |
| **Capa 4**: LLM Provider | `src/llm/` | `llm_provider.py`, `prompts.py` |
| **Capa 5**: Data Pipeline | `src/data_pipeline/` | `pdf_extractor.py`<br/>`text_cleaner.py`<br/>`chunker.py`<br/>`pipeline_orchestrator.py` |


## Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.10+
- [Ollama](https://ollama.ai/) instalado (para LLM local gratuito)
- 4 GB de RAM m√≠nimo (8 GB recomendado)

### Opci√≥n A: Instalaci√≥n manual

```bash
git clone https://github.com/<tu-usuario>/chatbot-lse-posgrados.git
cd chatbot-lse-posgrados
python -m venv venv
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

pip install -r requirements.txt
```

### Opci√≥n B: Docker Compose (recomendado para producci√≥n)

```bash
# Iniciar todos los servicios (Ollama + API + UI)
docker-compose up -d

# Ver logs
docker-compose logs -f api

# Ejecutar pipeline de datos
docker-compose run --rm pipeline

# Detener
docker-compose down
```

### Configurar el LLM

```bash
# Descargar modelo (elegir uno):
ollama pull llama3          # 4.7 GB - Recomendado
ollama pull llama3:8b       # Variante 8B
ollama pull mistral         # 4.1 GB - Alternativa
```

### Configurar variables de entorno

```bash
cp .env.example .env
# Editar .env seg√∫n tu configuraci√≥n
```

### Colocar documentos y ejecutar pipeline

```bash
# Colocar PDFs en data/raw/
python run_pipeline.py
```

### Lanzar la API y la interfaz

```bash
# Terminal 1: API
python run_api.py

# Terminal 2: Interfaz
python run_app.py
```

Acceder a:
- **Chatbot:** http://localhost:8501
- **Analytics:** http://localhost:8501/analytics
- **API Docs:** http://localhost:8000/docs

## Uso

### Ejemplos de consultas

```
# Preguntas factuales
"¬øCu√°l es el porcentaje m√≠nimo de asistencia requerido?"
"¬øQu√© t√≠tulo otorga la CEIA?"
"¬øCu√°ntos bimestres dura la especializaci√≥n?"

# Preguntas procedimentales
"¬øC√≥mo me inscribo en Gesti√≥n de Proyectos?"
"¬øQu√© tengo que hacer para solicitar una pr√≥rroga?"
"¬øC√≥mo es el proceso de defensa del trabajo final?"

# Preguntas comparativas (mejor con Hybrid/GraphRAG)
"¬øCu√°l es la diferencia entre MIAE y MIA?"
"¬øQu√© maestr√≠as puedo hacer despu√©s de la CESE?"
"¬øCu√°les son los requisitos de la MIA y qu√© especializaci√≥n necesito?"

# Preguntas de contacto
"¬øA qui√©n contacto para dudas sobre inscripci√≥n?"
"¬øCu√°l es el email de gesti√≥n acad√©mica?"

# Preguntas con memoria conversacional
"¬øCu√°les son los requisitos de la CEIA?"  ‚Üí  (respuesta)
"¬øY cu√°ntos bimestres dura?"              ‚Üí  contextualiza autom√°ticamente a CEIA

# Preguntas fuera de dominio (abstenci√≥n correcta)
"¬øCu√°nto cuesta la carrera?" ‚Üí Abstenci√≥n + contacto de fallback
"¬øQu√© opin√°s sobre la UTN?" ‚Üí Fuera de alcance
```

### API REST

```bash
# Consulta simple
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "¬øCu√°l es la asistencia m√≠nima?", "mode": "hybrid"}'

# Consulta con memoria conversacional
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"question": "¬øY cu√°ntos bimestres dura?", "mode": "hybrid", "session_id": "sesion-1"}'

# Comparaci√≥n de m√©todos
curl -X POST http://localhost:8000/api/v1/chat/compare \
  -H "Content-Type: application/json" \
  -d '{"question": "¬øCu√°les son los requisitos de la MIA?"}'

# Enviar feedback
curl -X POST http://localhost:8000/api/v1/feedback \
  -H "Content-Type: application/json" \
  -d '{"question": "¬øCu√°l es la asistencia m√≠nima?", "answer": "75%", "rating": 5, "is_correct": true}'

# Ver estad√≠sticas de feedback
curl http://localhost:8000/api/v1/feedback/stats
```

## Evaluaci√≥n

### Ejecutar evaluaci√≥n completa (con RAGAS)

```bash
python run_evaluation.py
```

### Evaluaci√≥n r√°pida (5 preguntas)

```bash
python run_evaluation.py --quick
```

### Evaluar por categor√≠a

```bash
python run_evaluation.py --category factual
python run_evaluation.py --category procedural
python run_evaluation.py --category comparative
```

### M√©tricas incluidas

| M√©trica | Descripci√≥n |
|---|---|
| **Keyword Hit Rate** | Porcentaje de palabras clave esperadas en la respuesta |
| **RAGAS Faithfulness** | Claims de la respuesta respaldados por el contexto |
| **RAGAS Answer Relevance** | Relevancia sem√°ntica respuesta-pregunta |
| **RAGAS Context Precision** | Porcentaje de contextos recuperados relevantes |
| **RAGAS Context Recall** | Cobertura de informaci√≥n necesaria en contextos |
| **Source Accuracy** | Coincidencia de fuentes esperadas vs recuperadas |
| **Abstenci√≥n correcta** | Detecci√≥n de preguntas fuera de dominio |
| **Tiempo de respuesta** | Latencia en milisegundos por m√©todo |

El reporte se genera en `data/evaluation/evaluation_report.json` y se visualiza en el dashboard de analytics.

### Benchmark de referencia: RAG vs GraphRAG vs Hybrid

| Tipo de pregunta | Mejor m√©todo | Raz√≥n |
|---|---|---|
| Datos espec√≠ficos (nota m√≠nima, plazos) | RAG | Informaci√≥n textual directa en los documentos |
| Relaciones entre programas (requisitos) | GraphRAG | Navegaci√≥n por entidades y relaciones en el grafo |
| Comparaciones entre carreras | Hybrid | Combina texto descriptivo + estructura relacional |
| Contactos y emails | RAG | Datos puntuales en documentos FAQ |
| Caminos de formaci√≥n (CESE ‚Üí maestr√≠a) | GraphRAG | Paths entre nodos del grafo |
| Requisitos + descripci√≥n completa | Hybrid | Necesita ambas fuentes de informaci√≥n |

## Tests

```bash
# Todos los tests
pytest

# Tests r√°pidos (sin modelos ML)
pytest -m "not slow"

# Tests por m√≥dulo
pytest tests/test_data_pipeline/
pytest tests/test_rag/
pytest tests/test_graph_rag/
pytest tests/test_hybrid/
pytest tests/test_api/
```

## Agregar nuevos documentos

1. Colocar el nuevo PDF en `data/raw/`
2. Ejecutar: `python run_pipeline.py --doc nombre_del_archivo.pdf`
3. El pipeline procesar√° solo el nuevo documento (procesamiento incremental)

Para forzar reprocesamiento completo:
```bash
python run_pipeline.py --force
```

## T√©cnicas avanzadas implementadas

### HyDE (Hypothetical Document Embeddings)

Basado en [Gao et al., 2022]. En lugar de buscar directamente por la query del usuario, el sistema:
1. Genera un "documento hipot√©tico" con el LLM que responde la pregunta
2. Usa el embedding de ese documento hipot√©tico para buscar en FAISS
3. Fusiona el embedding HyDE con el embedding directo (alpha configurable)
4. Re-rankea contra la query original para mantener relevancia

Esto mejora el retrieval porque el documento hipot√©tico tiene vocabulario m√°s similar a los documentos reales que la query del usuario.

### Query Expansion

El sistema expande cada consulta de tres formas:
1. **Sin√≥nimos del dominio:** Diccionario espec√≠fico del LSE-FIUBA (ej: "requisito" ‚Üí "condici√≥n", "materia" ‚Üí "asignatura")
2. **Reformulaciones LLM:** Genera 3 variantes de la pregunta con diferentes palabras clave
3. **Fusi√≥n de resultados:** Combina y re-rankea resultados de todas las variantes

### Memoria conversacional

- **Ventana deslizante:** Mantiene los √∫ltimos N turnos de conversaci√≥n
- **Resumen progresivo:** Comprime turnos viejos en un resumen con LLM
- **Contextualizaci√≥n:** Detecta pronombres y referencias anaf√≥ricas, reformula la query para que sea autocontenida
- **Tracking de t√≥picos:** Identifica programas y temas discutidos en la sesi√≥n

### Anti-alucinaci√≥n multi-capa

7 capas de protecci√≥n:
1. Verificaci√≥n de fidelidad por embeddings (similitud claim-contexto)
2. Verificaci√≥n de fidelidad por LLM (an√°lisis de claims)
3. Verificaci√≥n heur√≠stica (matching de datos espec√≠ficos)
4. Cross-referencing RAG-GraphRAG (consistencia entre fuentes)
5. Abstenci√≥n inteligente (confianza baja o fuera de dominio)
6. Contactos de fallback (sugiere emails relevantes)
7. Citaciones obligatorias (trazabilidad a fuentes)

## Casos de fallo conocidos y limitaciones

### Limitaciones del sistema

| Limitaci√≥n | Descripci√≥n | Mitigaci√≥n |
|---|---|---|
| **Dependencia de LLM** | La calidad depende del modelo LLM disponible | Fallback heur√≠stico cuando LLM no est√° disponible |
| **Cobertura de documentos** | Solo responde sobre los 13 PDFs del corpus | Abstenci√≥n + contacto de fallback para preguntas no cubiertas |
| **Idioma** | Optimizado para espa√±ol rioplatense | Embeddings multiling√ºes, pero prompts en espa√±ol |
| **Actualizaci√≥n manual** | Los documentos deben actualizarse manualmente | Pipeline incremental con detecci√≥n de cambios SHA-256 |
| **Latencia** | Embedding + LLM puede tomar 2-10 segundos | Cross-encoder reranking agrega latencia pero mejora precisi√≥n |
| **Informaci√≥n de costos** | No maneja informaci√≥n de aranceles | Abstenci√≥n correcta para preguntas de costos |

### Casos de fallo documentados

1. **Preguntas ambiguas sin programa:** Cuando el usuario pregunta "¬øcu√°les son los requisitos?" sin especificar programa, el sistema puede mezclar informaci√≥n de m√∫ltiples carreras.
   - *Mitigaci√≥n:* Usar filtro por programa en la UI o clarificar en la pregunta.

2. **Preguntas sobre regulaciones muy recientes:** Si el reglamento cambi√≥ despu√©s de los PDFs procesados, la informaci√≥n puede estar desactualizada.
   - *Mitigaci√≥n:* Re-ejecutar pipeline cuando se actualicen documentos.

3. **Preguntas multi-hop complejas:** Consultas que requieren razonar sobre m√°s de 3 saltos en el grafo pueden perder contexto.
   - *Mitigaci√≥n:* GraphRAG con profundidad configurable; complementar con RAG.

4. **Tablas complejas en PDFs:** Algunas tablas de planes de estudio con formatos irregulares pueden no extraerse perfectamente.
   - *Mitigaci√≥n:* Extracci√≥n dual PyMuPDF + pdfplumber con fallback.

5. **Preguntas en ingl√©s:** El sistema responde en espa√±ol aunque se pregunte en ingl√©s; la calidad de retrieval puede disminuir.
   - *Mitigaci√≥n:* Embeddings multiling√ºes ayudan parcialmente.

### Evoluci√≥n futura

- Fine-tuning del modelo de embeddings para el dominio acad√©mico
- Graph Neural Networks para node embeddings m√°s expresivos
- Soporte multimodal (diagramas y tablas de los PDFs)
- Active learning con el feedback recolectado
- Migraci√≥n a microservicios para escalabilidad

## Stack tecnol√≥gico

| Componente | Tecnolog√≠a |
|---|---|
| LLM | Ollama (llama3) / OpenAI API |
| Embeddings | sentence-transformers (multilingual-MiniLM-L12-v2, 384 dims) |
| Vector DB | FAISS (IndexFlatIP) |
| Graph DB | NetworkX + python-louvain |
| Query Enhancement | HyDE + Query Expansion + Cross-Encoder Reranking |
| Anti-alucinaci√≥n | Faithfulness check + Cross-reference + Abstenci√≥n |
| Memoria | Ventana deslizante + Resumen progresivo |
| Evaluaci√≥n | RAGAS (Faithfulness, Answer Relevance, Context Precision, Recall) |
| Feedback | Human-in-the-Loop con almacenamiento JSON |
| API | FastAPI + uvicorn |
| UI | Streamlit (chat + analytics dashboard) |
| PDF Processing | PyMuPDF + pdfplumber |
| Deployment | Docker Compose (Ollama + API + UI) |
| Testing | pytest |

## Documentos procesados

| Documento | Tipo | Descripci√≥n |
|---|---|---|
| CEIA.pdf | Resoluci√≥n | Plan de estudios - Esp. en Inteligencia Artificial |
| CESE.pdf | Resoluci√≥n | Plan de estudios - Esp. en Sistemas Embebidos |
| CEIoT.pdf | Resoluci√≥n | Plan de estudios - Esp. en Internet de las Cosas |
| MIAE.pdf | Resoluci√≥n | Plan de estudios - Maestr√≠a en IA Embebida |
| MIoT.pdf | Resoluci√≥n | Plan de estudios - Maestr√≠a en IoT |
| MCB.pdf | Resoluci√≥n | Plan de estudios - Maestr√≠a en Ciberseguridad |
| MIA-AE1-Programa.pdf | Programa | Programa de materia MIA |
| Reglamento...2025.pdf | Reglamento | Reglamento de cursada y asistencia |
| FAQ - MIA.pdf | FAQ | Preguntas frecuentes MIA |
| FAQ - GdP...pdf | FAQ | Preguntas frecuentes GdP, GTI, TTFA, TTFB |
| FAQ - Optativas.pdf | FAQ | Preguntas frecuentes materias optativas |
| LSE-FIUBA-Trabajo-Final.pdf | Reglamento | Reglamento de trabajo final |
| Programa de Vinculaci√≥n.pdf | Vinculaci√≥n | Programa de vinculaci√≥n profesional |


---
**Laboratorio de Sistemas Embebidos (LSE)** - Facultad de Ingenier√≠a - Universidad de Buenos Aires
